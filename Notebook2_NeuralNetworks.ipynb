{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "\n",
    "Adapted from:\n",
    "* https://towardsdatascience.com/machine-learning-for-beginners-an-introduction-to-neural-networks-d49f22d238f9\n",
    "* https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are Neural Networks?\n",
    "\n",
    "Neural networks have become an extremely popular tool in AI and machine learning. At its core, neural networks are simply a series of mathematical operations applied to some given inputs. The term \"neural network\" comes from the fact that each one of these mathematical operations can be thought of as a neuron firing in a brain.\n",
    "\n",
    "### Neurons\n",
    "\n",
    "Following this brain analogy, we will first talk about the basic building block on a neural network: the \"neuron\". A neuron takes a multitude of different numbers, combines these inputs, then spits out a single number. A visualization of a neuron is shown below.\n",
    "\n",
    "<img src=\"img/neuron.png\">\n",
    "\n",
    "There are three steps to computing the final output for a neuron:\n",
    "\n",
    "1. Multiply the inputs by weights (this is represented by the red blocks in the picture). For example, in the picture above there are two inputs $x_1$ and $x_2$. The neuron keeps track of corresponding weights $w_1$ and $w_2$, and in the first step we multiply $x_1$ by $w_1$ and $x_2$ by $w_2$.\n",
    "\n",
    "2. Add the weighted inputs and bias together (green block). Along with the weights, the neuron keeps track of a bias term we will refer to as $b$. The second step is then to compute $w_1 * x_1 + w_2 * x_2 + b$.\n",
    "\n",
    "3. Apply an _activation_ function (yellow block). In this last step, we can apply any arbitrary function $f$. That is, we compute $f(w_1 * x_1 + w_2 * x_2 + b)$. There are several popular choices of functions that we will discuss later.\n",
    "\n",
    "### Playing around with Neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start we will consider a neuron that takes a single input. The activation function that we will consider is the step function $f$ defined to be,\n",
    "\n",
    "$$\n",
    "f(x) = \\begin{cases}\n",
    "1 & x \\geq 0 \\\\\n",
    "0 & x < 0 \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "In other words, $f(x)$ is 1 if $x$ is greater than or equal to $0$ and is $0$ if $x$ is negative. Suppose that we wish to design our neuron so that it outputs 1 if $x_1 \\leq -2$ and $0$ if $x_1 > -2$. Fill in the missing weight and bias that achieves this neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 1.000000\t Code Output: 1.000000\t Expected: 0.000000\tIncorrect :(\n"
     ]
    }
   ],
   "source": [
    "# TODO: Find the correct w_1 and b!\n",
    "w_1 = 0\n",
    "b = 0\n",
    "\n",
    "def neuron(x_1):\n",
    "    # Step 1: Multiply weight.\n",
    "    step1 = w_1 * x_1\n",
    "    # Step 2: Add bias.\n",
    "    step2 = step1 + b\n",
    "    # Step 3: Apply activation function.\n",
    "    step3 = activation(step2)\n",
    "    return step3\n",
    "\n",
    "def activation(z):\n",
    "    if z >= 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def tester(x_1, correct_val):\n",
    "    \"\"\"Helper function to check your work.\n",
    "    Args:\n",
    "        x_1: Input to the neuron.\n",
    "        correct_val: What the correct value should be.\n",
    "    \"\"\"\n",
    "    output = neuron(x_1)\n",
    "    print 'Input: %f\\t Code Output: %f\\t Expected: %f\\t' % (x_1, output, correct_val),\n",
    "    if correct_val == output:\n",
    "        print 'Correct!'\n",
    "    else:\n",
    "        print 'Incorrect :('\n",
    "    \n",
    "# TODO: Write tests to check your weight and bias. We have provided one for you.\n",
    "tester(1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now suppose that we want to create a neuron that encodes the AND function for three variables. That is, we have inputs $x_1$, $x_2$, $x_3$, all of which can either be $0$ or $1$, and we want the neuron to output $1$ only if all of theses inputs are $1$. Code this neurons and find the appropriate weights and bias below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Find the correct w_1, w_2, w_3, and b!\n",
    "w_1 = 0\n",
    "w_2 = 0\n",
    "w_3 = 0\n",
    "b = 0\n",
    "\n",
    "def neuron(x_1, x_2, x_3):\n",
    "    # TODO: Code the neuron!\n",
    "    # Step 1: Multiply by weights.\n",
    "    # Step 2: Add weighted inputs and bias together.\n",
    "    # Step 3: Apply activation function.\n",
    "    pass\n",
    "\n",
    "def activation(z):\n",
    "    if z >= 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def tester(x_1, x_2, x_3, correct_val):\n",
    "    \"\"\"Helper function to check your work.\n",
    "    Args:\n",
    "        x_1: Input to the neuron.\n",
    "        correct_val: What the correct value should be.\n",
    "    \"\"\"\n",
    "    output = neuron(x_1)\n",
    "    print 'Input: %f\\t Code Output: %f\\t Expected: %f\\t' % (x_1, output, correct_val),\n",
    "    if correct_val == output:\n",
    "        print 'Correct!'\n",
    "    else:\n",
    "        print 'Incorrect :('\n",
    "\n",
    "tester(1, 1, 1, 1)\n",
    "tester(1, 1, 0, 0)\n",
    "tester(1, 0, 1, 0)\n",
    "tester(0, 1, 1, 0)\n",
    "tester(0, 0, 0, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Composing Neurons Together\n",
    "\n",
    "Now that we understand a bit about neurons, a neural network is simply multiple neurons joined together. A pictoral example of a neural network is shown below. We usually organize neural networks by layers, where each layer has a number of neurons.\n",
    "\n",
    "<img src=\"img/simple_nn.png\">\n",
    "\n",
    "Here the two white circles show two inputs $x_1$ and $x_2$. These inputs are both fed into two separate neurons, $h_1$ and $h_2$. The results from each of the neurons are then fed into a final neuron, $o_1$. Let's try coding this example neural network! Each of the neurons have been coded below. Use these implementations to create the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 Test Cases Correct.\n"
     ]
    }
   ],
   "source": [
    "def neural_net(x_1, x_2):\n",
    "    # TODO: Implement the neural network. The output of this function should be the output of the neural network.\n",
    "    pass\n",
    "\n",
    "def neuron_h1(x_1, x_2):\n",
    "    w_1 = 3\n",
    "    w_2 = 1\n",
    "    b = -2\n",
    "    return activation(w_1 * x_1 + w_2 * x_2 + b)\n",
    "\n",
    "def neuron_h2(x_1, x_2):\n",
    "    w_1 = 0\n",
    "    w_2 = 3\n",
    "    b = 1\n",
    "    return activation(w_1 * x_1 + w_2 * x_2 + b)\n",
    "\n",
    "def neuron_o1(h_1, h_2):\n",
    "    w_1 = -5\n",
    "    w_2 = 3\n",
    "    b = 1\n",
    "    return activation(w_1 * h_1 + w_2 * h_2 + b)\n",
    "\n",
    "def activation(z):\n",
    "    # Here we will use the \"sigmoid\" function for our activation.\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "num_correct = 3\n",
    "if neural_net(1, 1) != 0.38747367270010746:\n",
    "    num_correct -= 1\n",
    "if neural_net(3, -1) != 0.025830530969362966:\n",
    "    num_correct -= 1\n",
    "if neural_net(2, 0) != 0.15227177022746463:\n",
    "    num_correct -= 1\n",
    "print '%d/3 Test Cases Correct.' % num_correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Neural Networks with Images\n",
    "\n",
    "Relating this back to our self-driving car application, we would like to use neural networks to recognize different objects while driving. In the next notebook, we will actually make a neural network that can recognize different street signs. However, for now, we will go over special layers that will help us make such a neural net."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Layer\n",
    "\n",
    "The _output layer_ is the final layer of the neural network. For our application, the output layer will have as many neurons as the number of different things we want to recognize. We apply the so-called _soft max_ function to the outputs of these neurons to transform them into probabilities. These probabilities correspond the the likelihood that the image corresponds to one of the objects we are looking for.\n",
    "\n",
    "For example, suppose that we want to tell whether we have a picture of a stop sign, a yield sign, or a pedestrian crossing sign. Since there are three different signs we are looking for, there will be three neurons. After applying the soft max function, the transformed output of the first neuron is the probability that the image is of a stop sign, the second is the probability it is a yield sign, and the third is the probability it is a pedestring crossing sign.\n",
    "\n",
    "To calculate the soft max of the neuron outputs, we exponentiate all outputs and then normalize by dividing by the sume. That is, if we have $K$ outputs, $x_1, x_2,\\ldots x_K$, then the $i^{th}$ value after doing soft max is:\n",
    "$$\n",
    "\\frac{e^{x_i}}{\\sum_{j=1}^K e^{x_j}}\n",
    "$$\n",
    "\n",
    "In the below exercise, we are given three different output values in the final layer (e.g. say stop sign, yield sign, pedestrian crossing sign). Implement the soft max function to convert these values into probabilities. Which sign should the network guess is in the image? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_layer(neuron_outputs, sign_names):\n",
    "    \"\"\"TODO: Return the probabilities and the name of the sign the network should guess.\n",
    "    Args:\n",
    "        neuron_outputs: List of neuron outputs.\n",
    "        sign_names: List of sign names. The i^th name in the list corresponds\n",
    "            to the i^th value in the neuron_outputs.\n",
    "    Returns: List of probabilities and the name of the sign the network should guess.\n",
    "    \"\"\"\n",
    "    probabilities = []\n",
    "    guess = ''\n",
    "    # TODO: YOUR CODE HERE!\n",
    "    return probabilities, guess\n",
    "\n",
    "probs, guess = output_layer([-3, 2, 1], ['Stop', 'Yield', 'Pedestrian'])\n",
    "print 'Probabilities:', probs, ' Guess:', guess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution Layer and Max Pooling Layer\n",
    "\n",
    "When dealing with picture data, one can drastically improve performance by including a convolution layer. This layer simply applies the convolution operation that we saw in the last notebook, and it produces a _channel_. Another way of thinking about channels is by thinking of them as the _depth_ of the image. For example, standard input images have three channels: red, blue, and green. The image below shows what it looks like to apply a single kernel across the image.\n",
    "\n",
    "<img src=\"img/convolusion_kernel.png\">\n",
    "\n",
    "Each convolution layer may be composed of many different kernels, each producing its own channel. Therefore, if we have six different kernels in a particular convolution layer, the image that is output will have six channels (i.e. a depth of six)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`#TODO: Suppose we have an input image with three channels and is 32 x 32 pixels. If our convolution layer has two kernels each with dimension 5x5 (with depth this is 3x5x5). What is the dimension of the output image?`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically, a convolution layer is immediately followed by a _pooling layer_. In particular, we will use _max pooling layers_. The max pooling operation is similar to the convolution operation, but instead of taking a weighted average of weights, we simply take the max value. A max pooling layer does this for every channel of the input image, and therefore the output image has the same number of channels. Below is a demonstration of how the max pooling operation is applied to a single channel. The blue image on the left shows the output, and the image on the right shows the corresponding input.\n",
    "\n",
    "![SegmentLocal](img/max_pool.gif \"segment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TODO: Suppose we pass the output from the last question as input into a max pooling layer where the kernel is 3x3. What is the dimension of the output`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our neural network may have several of these convolutional/pooling layers. However, before we pass the output from these layers to a _fully connected layer_ (i.e. a layer composed of neurons like we first saw and where each neuron takes in every output from the last layer as its inputs), we must _flatten_ or _unroll_ the output. This operation is simply just rearranging the values in a matrix into a vector (see example below).\n",
    "\n",
    "<img src=\"img/unroll.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`#TODO: Suppose that we want to pass the output from the last max pooling layer to a fully connected layer. How many inputs will there be to each neuron? Check your answers with an instructor before moving on.`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
