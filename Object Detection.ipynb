{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 3: Intro to Object Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-825906d5a6be>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import copy\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import imageio\n",
    "import pickle as pkl\n",
    "from PIL import Image\n",
    "from IPython import display\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Vechicle Detection\n",
    "\n",
    "So far, we have covered the basic task of image recognition. In this notebook, we will explore how to train computers to not only classify images, but also detect objects in an image as well. We will first cover the basics of bounding boxes and then learn how to adapt pretrained classification models to build a vechicle classifiers. We then apply this classifier to bounding boxes using sliding window and selective search techniques to create a simple object detector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sliding Windows\n",
    "\n",
    "We will be working with the following image of a city road, which we will apply objection detection to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'notebook4_images/img.jpg'\n",
    "img = Image.open(fname)\n",
    "\n",
    "plt.figure(figsize = (10, 10))\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first introduce a simple sliding window technique in which we use slide through the image, looking at cropped images of a fixed fixed size. We provide for creating a gif that illustrates how this technique works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(image, stepSize, windowSize):\n",
    "    \"\"\"\n",
    "    Slides a window across an image\n",
    "\n",
    "    Args:\n",
    "        image (np array): Input image\n",
    "        stepSize (int): Step size for sliding window (both in x and y directions)\n",
    "        windowSize (int tuple): (window width, window height)\n",
    "\n",
    "    Yields:\n",
    "        yields x and y coordinates of the upper-lefthand corner and the cropped image (np array)\n",
    "    \"\"\"\n",
    "    \n",
    "    # slide a window across the image\n",
    "    for y in range(0, image.shape[0], stepSize):\n",
    "        for x in range(0, image.shape[1], stepSize):\n",
    "            # yield the current window\n",
    "            yield (x, y, image[y:y + windowSize[1], x:x + windowSize[0]])\n",
    "            \n",
    "\n",
    "def sliding_gif(image, gif_fname, stepSize=64, winW=128, winH=128, gif_scale=0.5):\n",
    "    \"\"\"\n",
    "    Saves a gif sliding windows on an image\n",
    "\n",
    "    Args:\n",
    "        image (np array): Input image\n",
    "        gif_fname (string): Save file name\n",
    "        winW (int): Window width\n",
    "        winH (int): Window height\n",
    "        gif_scale (float): Scale the size of the gif (smaller scale = faster processing)\n",
    "    \"\"\"\n",
    "    height, width, _ = image.shape\n",
    "    width_gif = int(width * gif_scale)\n",
    "    height_gif = int(height * gif_scale)\n",
    "\n",
    "    images = []\n",
    "\n",
    "    for (x, y, window) in sliding_window(image, stepSize=stepSize, windowSize=(winW, winH)):\n",
    "        # if the window does not meet our desired window size, ignore it\n",
    "        if window.shape[0] != winH or window.shape[1] != winW:\n",
    "            continue\n",
    "\n",
    "        clone = image.copy()\n",
    "        cv2.rectangle(clone, (x, y), (x + winW, y + winH), (0, 255, 0), 2)\n",
    "        clone = cv2.resize(clone, (width_gif, height_gif))\n",
    "        images.append(Image.fromarray(clone))\n",
    "\n",
    "    imageio.mimwrite(gif_fname, images, duration=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to play around with the step size and window dimensions\n",
    "stepSize = 50\n",
    "winW = 100\n",
    "winH = 60\n",
    "\n",
    "image = np.array(img)\n",
    "fname = \"notebook4_images/sliding.gif\"\n",
    "sliding_gif(image, fname, stepSize=stepSize, winW=winW, winH=winH)\n",
    "display.Image(filename=fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding image classification\n",
    "\n",
    "Now given a set of cropped images, we can perform image classification on each window. In the previous notebook, we had you build your own image recognition model. Here instead, we will use a pre-trained residual network (ResNet-50), which has been shown to do well on image classification tasks while being relatively lightweight. \n",
    "\n",
    "The diagram below plots the top-1% accuracy of popular image models on ImageNet against the operations required for an image to be passed through the network to generate a prediction. The size of each blob is proportional to the number of parameters of the neural network. Generally, larger models require more space and computational resources. As seen in the figure, ResNets generally offer a fairly good balance between performance and memory. For example, compare ResNet-50 to its predecessors, VGG-16 and VGG-19, which achieves worse accuracy and has much higher computation and memory costs.\n",
    "\n",
    "![compare models](notebook4_images/image_model_comparison.png)\n",
    "\n",
    "https://arxiv.org/abs/1605.07678"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load ResNet-50\n",
    "# We set the argument pretrained=True, which downloads the weights for the model\n",
    "# that are pretrained on the ImageNet database.\n",
    "model = models.resnet50(pretrained=True)\n",
    "model.eval() # will print out the layers of the model (feel free to ignore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first try classifying the van in the lower-lefthand corner. First find a bounding box around the van. We provide code below that will visualize the bounding box using your chosen inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Adjust the following values of the upper-lefthand and lower-righthand corners of the bounding box\n",
    "x1 = 0\n",
    "x2 = 100\n",
    "y1 = 300\n",
    "y2 = 375\n",
    "\n",
    "image = np.array(img)\n",
    "clone = image.copy()\n",
    "cv2.rectangle(clone, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "plt.figure(figsize = (10, 10))\n",
    "plt.imshow(clone)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will get the top 5 class labels for the cropped image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use the coordinates you settled on in the previous cell {x1, x2, y1, y2} to create a cropped image\n",
    "# Feel free to show the image using plt.imshow() to verify that you have cropped the image correctly\n",
    "cropped = image[y1:y2, x1:x2]\n",
    "\n",
    "plt.figure(figsize = (10, 10))\n",
    "plt.imshow(cropped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now run the image through our neural network to generate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts from a numpy array to a PIL image\n",
    "cropped = Image.fromarray(cropped)\n",
    "\n",
    "\"\"\"\n",
    "transform the image (standard for our ResNet model):\n",
    "1) resize the image to 256x256\n",
    "2) apply a center crop to get a final output size of 224x224 (which is the required input size for our ResNet architecture)\n",
    "3) convert the image to a pytorch Tensor format\n",
    "4) normalize the image channels\n",
    "\"\"\"\n",
    "transform = transforms.Compose([transforms.Resize([256,256]),\n",
    "                                transforms.CenterCrop(224),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "cropped = transform(cropped).unsqueeze(0)\n",
    "\n",
    "# run the image through our model and convert the output from a pytorch tensor to a numpy array format\n",
    "preds = model(cropped).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/imagenet_dict.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-bb469ce4a651>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# loads a dictionary that maps the label value to the class label name\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/imagenet_dict.pkl'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mimagenet_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpkl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#TODO: Get the top 5 scoring labels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/imagenet_dict.pkl'"
     ]
    }
   ],
   "source": [
    "# loads a dictionary that maps the label value to the class label name\n",
    "with open('data/imagenet_dict.pkl', 'rb') as handle:\n",
    "    imagenet_dict = pkl.load(handle)\n",
    "\n",
    "#TODO: Get the top 5 scoring labels and class label names (using imagenet_dict)\n",
    "k = 5\n",
    "top_k = preds.argsort()[0][-k:][::-1]\n",
    "\n",
    "for i, idx in enumerate(top_k):\n",
    "    print(i + 1, imagenet_dict[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tackling our problem\n",
    "\n",
    "Having access to this pretrained image classification model,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = 0\n",
    "x2 = 100\n",
    "y1 = 50\n",
    "y2 = 150\n",
    "\n",
    "image = np.array(img)\n",
    "clone = image.copy()\n",
    "cv2.rectangle(clone, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "plt.figure(figsize = (10, 10))\n",
    "plt.imshow(clone)\n",
    "\n",
    "transform = transforms.Compose([transforms.Resize([256,256]),\n",
    "                                transforms.CenterCrop(224),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "cropped = image[y1:y2, x1:x2]\n",
    "cropped = Image.fromarray(cropped)\n",
    "cropped = transform(cropped).unsqueeze(0)\n",
    "preds = model(cropped).detach().numpy()\n",
    "\n",
    "k = 5\n",
    "top_k = preds.argsort()[0][-k:][::-1]\n",
    "\n",
    "# loads a dictionary that maps the label value to the class label name\n",
    "with open('data/imagenet_dict.pkl', 'rb') as handle:\n",
    "    imagenet_dict = pkl.load(handle)\n",
    "    \n",
    "for i, idx in enumerate(top_k):\n",
    "    print(i + 1, imagenet_dict[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning\n",
    "\n",
    "Next, we wil use transfer learning to build a model that does binary classification with class labels: vehicle and no vehicle.\n",
    "\n",
    "Developing state-of-the-art models often requires vast amounts of data and computational resources. Moreover, even when given a specific model architecture, researchers often spend significant time tuning hyperparameters in order to push the performance of their model. As a result, it is not unheard of for research lab teams to have access to clusters with hundreds of GPUs. We however could not expect you to train from scratch a state-of-the-art image recognition model on a large image database, such as ImageNet, given the limited time and computational resources.\n",
    "\n",
    "To circumvent this issue, we introduce a popular approach called transfer learning, which is a method where a model adapted for some task is adapted as a starting point for another task.\n",
    "\n",
    "requiring that the model efficiently learn to extract features from photographs in order to perform well on the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR-100\n",
    "\n",
    "We use the CIFAR-100 dataset (see: https://www.cs.toronto.edu/~kriz/cifar.html), which has 100 classes containing 600 images each. There are 500 training images and 100 testing images per class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Again, we transform the images before feeding them into the network. We add an additional transformation called\n",
    "RandomHorizontalFlip. Every time the data loader feeds the image through the model, it will randomly flip the image\n",
    "with probability p=0.5. This improves training by essentially augmenting our image dataset with more images that have\n",
    "been slightly adjusted.\n",
    "\"\"\"\n",
    "trans_cifar_train = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
    "                                        transforms.Resize([256,256]),\n",
    "                                        transforms.CenterCrop(224),\n",
    "                                        transforms.ToTensor(),\n",
    "                                        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "# Same transform for the validation set, although since these are test examples, we don't want to flip them\n",
    "trans_cifar_val = transforms.Compose([transforms.Resize([256,256]),\n",
    "                                      transforms.CenterCrop(224),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "dataset_train = datasets.CIFAR100('data/cifar100', train=True, download=True, transform=trans_cifar_train)\n",
    "dataset_test = datasets.CIFAR100('data/cifar100', train=False, download=True, transform=trans_cifar_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# we create two dictionaries that map the class label name to the label number (and vice-versa)\n",
    "with open('data/cifar100/cifar-100-python/meta', 'rb') as handle:\n",
    "    classes = pkl.load(handle)['fine_label_names']\n",
    "\n",
    "class_to_idx = {}\n",
    "idx_to_class = {}\n",
    "for idx, class_name in enumerate(classes):\n",
    "    class_to_idx[class_name] = idx\n",
    "    idx_to_class[idx] = class_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modifying CIFAR for our needs\n",
    "\n",
    "We need to modify our dataset now so that we only have two classes, vehicle and not vehicle. Using the dictionaries you have loaded, look through the class names and find the relevant vehicle classes (Feel free to change this to a multilabel classification if you wish to distinguish between different types of vehicles). \n",
    "\n",
    "Note, you can directly modify the labels by changing the numpy arrays: dataset_train.train_labels and dataset_test.test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vehicle_classes = ['bus', 'motorcycle', 'pickup_truck', 'streetcar']\n",
    "vehicle_classes_idx = [class_to_idx[class_name] for class_name in vehicle_classes]\n",
    "person_classes = ['boy', 'girl', 'man', 'woman']\n",
    "person_classes_idx = [class_to_idx[class_name] for class_name in person_classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = dataset_train.train_labels\n",
    "train_labels = np.array(train_labels)\n",
    "\n",
    "mask_bus = train_labels == class_to_idx['bus']\n",
    "mask_motorcycle = train_labels == class_to_idx['motorcycle']\n",
    "mask_truck = train_labels == class_to_idx['pickup_truck']\n",
    "mask_car = train_labels == class_to_idx['streetcar']\n",
    "mask_background = np.invert(mask_bus | mask_motorcycle | mask_truck | mask_car)\n",
    "\n",
    "train_labels[mask_bus] = 1\n",
    "train_labels[mask_motorcycle] = 2\n",
    "train_labels[mask_truck] = 3\n",
    "train_labels[mask_car] = 4\n",
    "train_labels[mask_background] = 0\n",
    "\n",
    "dataset_train.train_labels = train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = dataset_test.test_labels\n",
    "test_labels = np.array(test_labels)\n",
    "\n",
    "mask_bus = test_labels == class_to_idx['bus']\n",
    "mask_motorcycle = test_labels == class_to_idx['motorcycle']\n",
    "mask_truck = test_labels == class_to_idx['pickup_truck']\n",
    "mask_car = test_labels == class_to_idx['streetcar']\n",
    "mask_background = np.invert(mask_bus | mask_motorcycle | mask_truck | mask_car)\n",
    "\n",
    "test_labels[mask_bus] = 1\n",
    "test_labels[mask_motorcycle] = 2\n",
    "test_labels[mask_truck] = 3\n",
    "test_labels[mask_car] = 4\n",
    "test_labels[mask_background] = 0\n",
    "\n",
    "dataset_test.test_labels = test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResnetCifar(nn.Module):\n",
    "    def __init__(self, num_classes=100):\n",
    "        super(ResnetCifar, self).__init__()\n",
    "        self.model = models.resnet50(pretrained=True)\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "        num_features = self.model.fc.in_features\n",
    "        self.model.fc = nn.Linear(num_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "def train_model(model, dataloaders, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_count = 0\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_count += len(labels)\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / running_count\n",
    "            epoch_acc = running_corrects.double() / running_count\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use GPU if it is available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# change num_classes if you have more than 2 classes\n",
    "num_classes = 2\n",
    "model = ResnetCifar(num_classes=num_classes)\n",
    "model = model.to(device)\n",
    "\n",
    "# create dictionary of dataloaders for train and test sets\n",
    "batch_size = 128\n",
    "dataloaders = {}\n",
    "dataloaders['train'] = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "dataloaders['val'] = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "\n",
    "w = mask_background.mean() * 100\n",
    "class_weights = np.ones(num_classes) * w\n",
    "class_weights[0] = 1.0\n",
    "class_weights = torch.FloatTensor(class_weights).cuda()\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "# use stocahstic gradient descent for optimization\n",
    "optimizer = optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001, momentum=0.9)\n",
    "\n",
    "# decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "# train and save the best model\n",
    "model = train_model(model, dataloaders, criterion, optimizer, exp_lr_scheduler, num_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_predictions(image, model, winW=128, winH=128, gif_scale=0.5):\n",
    "    height, width, _ = image.shape\n",
    "    width_gif = int(width * gif_scale)\n",
    "    height_gif = int(height * gif_scale)\n",
    "\n",
    "    for (x, y, window) in sliding_window(image, stepSize=64, windowSize=(winW, winH)):\n",
    "        # if the window does not meet our desired window size, ignore it\n",
    "        if window.shape[0] != winH or window.shape[1] != winW:\n",
    "            continue\n",
    "\n",
    "        clone = image.copy()\n",
    "        cropped = image[y:y + winH, x:x + winW]\n",
    "        cropped = Image.fromarray(cropped)\n",
    "        cropped = trans_cifar_val(cropped).unsqueeze(0)\n",
    "        cropped = cropped.to(device)\n",
    "        output = model(cropped).cpu().detach().numpy()\n",
    "        pred = np.argmax(output)\n",
    "        print(pred, output)\n",
    "        if pred != 0:\n",
    "            plt.imshow(image[y:y + winH, x:x + winW])\n",
    "            plt.show()\n",
    "\n",
    "winW = 100\n",
    "winH = 60\n",
    "sliding_predictions(image, model, winW=winW, winH=winH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
